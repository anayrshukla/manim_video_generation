{
  "d510bb22-641a-4717-a916-0e9eeb50b470": {
    "job_id": "d510bb22-641a-4717-a916-0e9eeb50b470",
    "status": "failed",
    "created_at": "2025-07-12T17:09:32.092509",
    "pdf_source": "uploads/ab374ecc-17e7-4316-b1f2-7e400f5dbd9f_2507.07980v1.pdf",
    "original_filename": "2507.07980v1.pdf",
    "video_name": null,
    "error": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'An unexpected error occurred. Please try again later.'}}"
  },
  "62793b6a-b4a9-44bd-be3e-8d89d1c2d56c": {
    "job_id": "62793b6a-b4a9-44bd-be3e-8d89d1c2d56c",
    "status": "completed",
    "created_at": "2025-07-12T17:14:57.583890",
    "pdf_source": "https://arxiv.org/pdf/2507.07980",
    "quality": "medium_quality",
    "video_name": null,
    "completed_at": "2025-07-12T17:15:31.514120",
    "video_path": "outputs/video_62793b6a-b4a9-44bd-be3e-8d89d1c2d56c.mp4",
    "generation_metrics": {
      "video_path": "outputs/video_62793b6a-b4a9-44bd-be3e-8d89d1c2d56c.mp4",
      "total_clips": 4,
      "successful_clips": 4,
      "failed_clips": 0,
      "success_rate": 1.0,
      "pdf_url": "https://arxiv.org/pdf/2507.07980",
      "clips_config": [
        {
          "type": "manim",
          "code": "class Scene1(Scene):\n    def construct(self):\n        robot = Rectangle(height=2, width=1, color=YELLOW)\n        touch = Dot(color=RED)\n        sensor = Text('Tactile Sensors: $1000+').scale(0.5)\n        x = Text('X').scale(2)\n        self.play(Create(robot))\n        self.play(Create(touch), Write(sensor))\n        self.play(Create(x), FadeOut(sensor))\n        self.play(Write(Text('What if robots could feel touch...without sensors?').scale(0.7)))",
          "voice_over": "Robots need to feel touch to interact naturally with humans. But adding touch sensors is expensive and complex. What if there was a better way?"
        },
        {
          "type": "manim",
          "code": "class Scene2(Scene):\n    def construct(self):\n        robot = Rectangle(height=2, width=1, color=YELLOW)\n        joints = [Dot(color=BLUE) for _ in range(3)]\n        arrows = [Arrow(joints[i], joints[i+1], color=WHITE) for i in range(2)]\n        brain = Circle(color=GREEN).scale(0.5).next_to(robot, UP)\n        self.play(Create(robot), *[Create(j) for j in joints], *[Create(a) for a in arrows])\n        self.play(Create(brain), Write(Text('Joint Sensors \u2192 Touch Location').scale(0.6)))",
          "voice_over": "UniTac uses the robot's existing joint sensors to detect touch. Like how you can tell where something touches your arm just by feeling how your muscles and joints react."
        },
        {
          "type": "manim",
          "code": "class Scene3(Scene):\n    def construct(self):\n        neural = Rectangle(width=3, height=2, color=PURPLE)\n        data = [Dot(color=BLUE) for _ in range(6)]\n        output = Dot(color=RED)\n        arrows = [Arrow(d, output, color=WHITE) for d in data]\n        self.play(Create(neural), *[Create(d) for d in data])\n        self.play(*[Create(a) for a in arrows], Create(output))\n        self.play(Write(Text('7-8cm accuracy').scale(0.6).next_to(neural, DOWN)))",
          "voice_over": "A neural network learns patterns in joint sensor data to pinpoint touch location within 7-8 centimeters - all without any additional hardware."
        },
        {
          "type": "manim",
          "code": "class Scene4(Scene):\n    def construct(self):\n        robot = Rectangle(height=2, width=1, color=YELLOW)\n        human = Circle(color=BLUE).shift(LEFT*2)\n        touch = Dot(color=RED)\n        arrow = Arrow(human, robot)\n        text = Text('Natural Interaction').scale(0.7)\n        self.play(Create(robot), Create(human))\n        self.play(Create(arrow), Create(touch))\n        self.play(Write(text))",
          "voice_over": "This breakthrough makes human-robot interaction more natural and accessible - enabling robots to respond to touch just like animals do, at a fraction of the cost."
        }
      ]
    }
  },
  "5301b217-c8b2-44c6-9620-0c754ac03726": {
    "job_id": "5301b217-c8b2-44c6-9620-0c754ac03726",
    "status": "completed",
    "created_at": "2025-07-12T17:20:33.221322",
    "pdf_source": "uploads/2d324437-daeb-44c6-a1c3-b90d9cf7a635_2507.07980v1.pdf",
    "original_filename": "2507.07980v1.pdf",
    "video_name": null,
    "completed_at": "2025-07-12T17:21:17.627788",
    "video_path": "outputs/video_5301b217-c8b2-44c6-9620-0c754ac03726.mp4",
    "generation_metrics": {
      "video_path": "outputs/video_5301b217-c8b2-44c6-9620-0c754ac03726.mp4",
      "total_clips": 4,
      "successful_clips": 4,
      "failed_clips": 0,
      "success_rate": 1.0,
      "pdf_path": "uploads/2d324437-daeb-44c6-a1c3-b90d9cf7a635_2507.07980v1.pdf",
      "clips_config": [
        {
          "type": "manim",
          "code": "class Scene1(Scene):\n    def construct(self):\n        robot = Square(color=YELLOW).scale(1.5)\n        sensor = Text('NO TOUCH\\nSENSORS', font_size=24).set_color(RED)\n        cross = Cross(sensor)\n        joint = Circle(color=WHITE, radius=0.3).next_to(robot, DOWN)\n        arrow = Arrow(joint, robot, color=BLUE)\n\n        self.play(Create(robot))\n        self.play(Write(sensor))\n        self.play(Create(cross))\n        self.play(Create(joint), Create(arrow))",
          "voice_over": "Modern robots can walk, run and manipulate objects with incredible precision. But there's a major limitation - most can't feel touch because they lack expensive tactile sensors. What if we could give robots a sense of touch using only the sensors they already have?"
        },
        {
          "type": "manim",
          "code": "class Scene2(Scene):\n    def construct(self):\n        joint = Circle(color=WHITE).scale(0.5)\n        waves = [Line(start=joint.get_center(), end=joint.get_center() + RIGHT*i/2 + UP*np.sin(i/2), color=BLUE) for i in range(1,8)]\n        hand = Circle(color=YELLOW).scale(0.3).move_to(waves[-1].get_end())\n        equation = Text('Joint Data \u2192 Touch Location', font_size=36)\n\n        self.play(Create(joint))\n        for wave in waves:\n            self.play(Create(wave), run_time=0.2)\n        self.play(Create(hand))\n        self.play(Write(equation))",
          "voice_over": "UniTac's key insight is that when you touch a robot, its joint sensors detect subtle changes in position and torque. By analyzing these patterns, we can figure out exactly where the robot was touched - no special sensors needed."
        },
        {
          "type": "manim",
          "code": "class Scene3(Scene):\n    def construct(self):\n        network = Rectangle(height=2, width=3, color=GREEN)\n        data = [Dot(color=BLUE) for _ in range(6)]\n        VGroup(*data).arrange(RIGHT).next_to(network, LEFT)\n        output = Dot(color=RED).next_to(network, RIGHT)\n        arrows = [Arrow(d, network, color=WHITE) for d in data]\n        final_arrow = Arrow(network, output, color=WHITE)\n\n        self.play(Create(network))\n        self.play(*[Create(d) for d in data])\n        self.play(*[Create(a) for a in arrows])\n        self.play(Create(final_arrow), Create(output))",
          "voice_over": "A neural network learns to map joint sensor readings to touch locations. After just a few hours of training data, it can pinpoint touches within 8 centimeters anywhere on the robot's body at 2000 times per second."
        },
        {
          "type": "manim",
          "code": "class Scene4(Scene):\n    def construct(self):\n        robot = Square(color=YELLOW).scale(1.5)\n        actions = [Text('Pet', font_size=24), Text('Guide', font_size=24), Text('Direct', font_size=24)]\n        VGroup(*actions).arrange(DOWN).next_to(robot, RIGHT)\n        check = Text('\u2713', font_size=48, color=GREEN).next_to(robot, UP)\n\n        self.play(Create(robot))\n        for action in actions:\n            self.play(Write(action))\n        self.play(Write(check))",
          "voice_over": "This breakthrough enables more natural human-robot interaction - robots can now understand petting, guiding touches, and direct manipulation, just like animals do. And it works on any robot with joint sensors, democratizing touch sensing for robotics."
        }
      ]
    }
  },
  "06134678-cc56-4f89-979a-cad1801ac8a1": {
    "job_id": "06134678-cc56-4f89-979a-cad1801ac8a1",
    "status": "completed",
    "created_at": "2025-07-12T17:22:05.046815",
    "pdf_source": "uploads/f9d22dc4-720e-4056-b385-7367beb38072_2507.07980v1.pdf",
    "original_filename": "2507.07980v1.pdf",
    "video_name": null,
    "completed_at": "2025-07-12T17:22:42.818688",
    "video_path": "outputs/video_06134678-cc56-4f89-979a-cad1801ac8a1.mp4",
    "generation_metrics": {
      "video_path": "outputs/video_06134678-cc56-4f89-979a-cad1801ac8a1.mp4",
      "total_clips": 4,
      "successful_clips": 4,
      "failed_clips": 0,
      "success_rate": 1.0,
      "pdf_path": "uploads/f9d22dc4-720e-4056-b385-7367beb38072_2507.07980v1.pdf",
      "clips_config": [
        {
          "type": "manim",
          "code": "class Scene1(Scene):\n    def construct(self):\n        robot = Rectangle(height=3, width=2, color=YELLOW)\n        dots = VGroup(*[Dot(color=RED) for _ in range(5)]).arrange(RIGHT)\n        question = Text('How can robots feel touch?', color=WHITE).scale(0.8)\n        cross = Cross(Rectangle(height=1, width=2, color=BLUE).scale(0.5))\n        sensor_text = Text('No tactile sensors needed!', color=GREEN).scale(0.7)\n\n        self.play(Create(robot))\n        self.play(Create(dots), Write(question))\n        self.play(Create(cross))\n        self.play(Write(sensor_text))",
          "voice_over": "Imagine giving robots the ability to feel touch - but without adding any special sensors. Sounds impossible, right? Well, researchers have found a clever way to do exactly that."
        },
        {
          "type": "manim",
          "code": "class Scene2(Scene):\n    def construct(self):\n        joint = Circle(color=WHITE)\n        arrows = VGroup(*[Arrow(start=ORIGIN, end=direction) for direction in [UP, DOWN, LEFT, RIGHT]])\n        text = Text('Using existing joint sensors', color=BLUE).scale(0.7)\n        brain = Circle(color=PURPLE).scale(1.2)\n        formula = Text('\u2192 Touch Location', color=GREEN).scale(0.7)\n\n        self.play(Create(joint), Create(arrows))\n        self.play(Write(text))\n        self.play(Create(brain), Write(formula))",
          "voice_over": "The key insight is that when you touch a robot, its joint sensors already detect tiny changes in pressure and position. By analyzing these patterns, we can figure out exactly where the touch happened."
        },
        {
          "type": "manim",
          "code": "class Scene3(Scene):\n    def construct(self):\n        network = VGroup(*[Circle(color=BLUE) for _ in range(6)]).arrange_in_grid(2, 3)\n        lines = VGroup(*[Line(c1.get_center(), c2.get_center(), color=WHITE) \n                        for c1, c2 in zip(network[:-1], network[1:])])\n        data = Text('Joint Data', color=YELLOW).scale(0.6)\n        arrow = Arrow(LEFT, RIGHT, color=WHITE)\n        output = Dot(color=RED)\n\n        self.play(Create(network), Create(lines))\n        self.play(Write(data), GrowArrow(arrow))\n        self.play(Create(output))",
          "voice_over": "A neural network processes data from the robot's joints in real-time. It's been trained to recognize specific patterns that indicate where contact occurs, achieving accuracy within 8 centimeters."
        },
        {
          "type": "manim",
          "code": "class Scene4(Scene):\n    def construct(self):\n        robot_arm = Rectangle(height=3, width=0.5, color=YELLOW)\n        dog_robot = Rectangle(height=1.5, width=2, color=YELLOW)\n        hand = Circle(color=ORANGE).scale(0.3)\n        text = Text('Real-time touch detection', color=GREEN).scale(0.7)\n        speed = Text('2000 times per second!', color=BLUE).scale(0.6)\n\n        self.play(Create(robot_arm), Create(dog_robot))\n        self.play(Create(hand), Write(text))\n        self.play(Write(speed))",
          "voice_over": "This breakthrough enables robots to respond naturally to human touch - whether it's a robotic arm following instructions or a robot dog reacting to being petted. And it all happens 2000 times per second, using hardware that's already built in."
        }
      ]
    }
  },
  "fcfb589d-cbc7-477b-8787-7dbdbd7b4ee8": {
    "job_id": "fcfb589d-cbc7-477b-8787-7dbdbd7b4ee8",
    "status": "completed",
    "created_at": "2025-07-12T17:27:42.576837",
    "pdf_source": "uploads/16f63461-b0f2-44df-a6b3-0653e90089fa_2507.07980v1.pdf",
    "original_filename": "2507.07980v1.pdf",
    "video_name": null,
    "completed_at": "2025-07-12T17:28:14.189310",
    "video_path": "outputs/video_fcfb589d-cbc7-477b-8787-7dbdbd7b4ee8.mp4",
    "generation_metrics": {
      "video_path": "outputs/video_fcfb589d-cbc7-477b-8787-7dbdbd7b4ee8.mp4",
      "total_clips": 4,
      "successful_clips": 4,
      "failed_clips": 0,
      "success_rate": 1.0,
      "pdf_path": "uploads/16f63461-b0f2-44df-a6b3-0653e90089fa_2507.07980v1.pdf",
      "clips_config": [
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('UniTac: Touch Sensing Without Sensors', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        robot = Square(color=BLUE, side_length=2)\n        question = Text('?', font_size=72, color=RED).next_to(robot)\n        self.play(Create(robot), Write(question))\n        self.wait(2)",
          "voice_over": "Most robots today can't feel touch because they lack expensive tactile sensors. But what if robots could sense touch using just their built-in joint sensors?"
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('Using Joint Sensors for Touch', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        joint = Circle(radius=0.5, color=BLUE)\n        arrow = Arrow(start=LEFT*2, end=RIGHT*2, color=WHITE)\n        contact = Dot(color=RED).shift(RIGHT*2)\n        self.play(Create(joint), Create(arrow), Create(contact))\n        self.wait(2)",
          "voice_over": "UniTac uses data from existing joint sensors to detect and locate touch anywhere on the robot's body, without adding any new hardware."
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('How UniTac Works', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        data = Rectangle(width=3, height=1.5, color=BLUE).shift(LEFT*2)\n        model = Circle(color=GREEN)\n        output = Rectangle(width=1, height=1, color=RED).shift(RIGHT*2)\n        self.play(Create(data), Create(model), Create(output))\n        self.wait(2)",
          "voice_over": "The system analyzes patterns in joint sensor data using machine learning to precisely locate where contact occurs on the robot's surface."
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('Real-World Impact', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        accuracy = Text('7-8cm Accuracy', color=GREEN).shift(UP*0.5)\n        speed = Text('2000Hz Speed', color=BLUE).shift(DOWN*0.5)\n        self.play(Write(accuracy), Write(speed))\n        self.wait(2)",
          "voice_over": "This enables robots to safely interact through touch with remarkable precision - localizing contact within 8 centimeters at 2000 times per second."
        }
      ]
    }
  },
  "9d74d549-c9bd-41e9-9c86-aea7ddaaefcf": {
    "job_id": "9d74d549-c9bd-41e9-9c86-aea7ddaaefcf",
    "status": "completed",
    "created_at": "2025-07-12T18:11:01.567034",
    "pdf_source": "uploads/8f9d556a-1e39-4954-901c-50f5211a77ef_2507.07980v1.pdf",
    "original_filename": "2507.07980v1.pdf",
    "video_name": null,
    "completed_at": "2025-07-12T18:11:36.261649",
    "video_path": "outputs/video_9d74d549-c9bd-41e9-9c86-aea7ddaaefcf.mp4",
    "generation_metrics": {
      "video_path": "outputs/video_9d74d549-c9bd-41e9-9c86-aea7ddaaefcf.mp4",
      "total_clips": 4,
      "successful_clips": 4,
      "failed_clips": 0,
      "success_rate": 1.0,
      "pdf_path": "uploads/8f9d556a-1e39-4954-901c-50f5211a77ef_2507.07980v1.pdf",
      "clips_config": [
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('Touch Sensing Without Sensors', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        robot = Square(color=BLUE, side_length=2).move_to(ORIGIN)\n        question = Text('?', font_size=72, color=RED).next_to(robot, RIGHT)\n        self.play(Create(robot), Write(question))\n        self.wait(2)",
          "voice_over": "Current robots lack a crucial ability - the sense of touch. Adding touch sensors is expensive and complex. But what if robots could feel touch without any special sensors?"
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('Using Built-in Joint Sensors', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        circle = Circle(radius=1, color=BLUE).move_to(ORIGIN)\n        dots = VGroup(*[Dot(color=YELLOW) for _ in range(6)]).arrange_in_grid(3,2)\n        self.play(Create(circle), Create(dots))\n        self.wait(2)",
          "voice_over": "UniTac uses the robot's existing joint sensors to detect touch. By analyzing changes in joint readings, we can determine where contact occurs on the robot's body."
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('Contact Localization', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        rect = Rectangle(height=3, width=2, color=BLUE).move_to(ORIGIN)\n        arrow = Arrow(start=LEFT*3, end=rect.get_left(), color=RED)\n        self.play(Create(rect), Create(arrow))\n        self.wait(2)",
          "voice_over": "When someone touches the robot, the contact creates unique patterns in joint sensor readings. Our system processes these patterns to pinpoint the touch location within 8 centimeters."
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('Enabling Natural Interaction', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        circle1 = Circle(radius=0.5, color=BLUE).shift(LEFT*2)\n        circle2 = Circle(radius=0.5, color=BLUE).shift(RIGHT*2)\n        arrow = Arrow(circle1.get_right(), circle2.get_left(), color=GREEN)\n        self.play(Create(circle1), Create(circle2), Create(arrow))\n        self.wait(2)",
          "voice_over": "This technology enables more natural human-robot interaction, allowing robots to respond to touch-based commands and physical guidance without additional hardware."
        }
      ]
    }
  },
  "34c1b950-f602-40a5-b605-f844319fec13": {
    "job_id": "34c1b950-f602-40a5-b605-f844319fec13",
    "status": "completed",
    "created_at": "2025-07-12T18:16:46.478114",
    "pdf_source": "uploads/f9148bbb-10e5-401a-9093-df1764116e71_2507.07980v1.pdf",
    "original_filename": "2507.07980v1.pdf",
    "video_name": null,
    "completed_at": "2025-07-12T18:18:11.415505",
    "video_path": "outputs/video_34c1b950-f602-40a5-b605-f844319fec13.mp4",
    "generation_metrics": {
      "video_path": "outputs/video_34c1b950-f602-40a5-b605-f844319fec13.mp4",
      "total_clips": 4,
      "successful_clips": 4,
      "failed_clips": 0,
      "success_rate": 1.0,
      "pdf_path": "uploads/f9148bbb-10e5-401a-9093-df1764116e71_2507.07980v1.pdf",
      "clips_config": [
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('UniTac: Touch Sensing Without Sensors', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        robot = Square(color=BLUE, side_length=2).move_to(ORIGIN)\n        question = Text('?', font_size=72, color=RED).next_to(robot, RIGHT)\n        self.play(Create(robot), FadeIn(question))\n        self.wait(2)",
          "voice_over": "Current robots lack a critical ability - the sense of touch. While humans and animals can naturally feel contact, most robots cannot detect when and where they're being touched."
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('Using Built-in Joint Sensors', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        circle = Circle(radius=1, color=BLUE).move_to(ORIGIN)\n        dots = VGroup(*[Dot(color=YELLOW) for _ in range(4)]).arrange_in_grid(2,2, buff=1)\n        self.play(Create(circle), Create(dots))\n        self.wait(2)",
          "voice_over": "UniTac uses the robot's existing joint sensors to detect touch, without requiring any additional hardware. It works by analyzing subtle changes in joint readings when contact occurs."
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('How UniTac Works', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        rect = Rectangle(height=2, width=3, color=BLUE).move_to(ORIGIN)\n        arrow = Arrow(start=LEFT*3, end=RIGHT*3, color=GREEN)\n        self.play(Create(rect), Create(arrow))\n        self.wait(2)",
          "voice_over": "When someone touches the robot, it creates unique patterns in the joint sensor readings. A neural network analyzes these patterns to determine the exact location of contact."
        },
        {
          "type": "manim",
          "code": "class SimpleScene(Scene):\n    def construct(self):\n        title = Text('Making Robots More Interactive', font_size=48).to_edge(UP)\n        self.play(Write(title))\n        self.wait(1)\n        circle1 = Circle(radius=0.5, color=BLUE).shift(LEFT*2)\n        circle2 = Circle(radius=0.5, color=BLUE).shift(RIGHT*2)\n        arrow = Arrow(circle1.get_right(), circle2.get_left(), color=GREEN)\n        self.play(Create(circle1), Create(circle2), Create(arrow))\n        self.wait(2)",
          "voice_over": "This breakthrough enables more natural human-robot interaction, allowing robots to respond to touch-based commands and better understand physical contact with their environment."
        }
      ]
    }
  }
}